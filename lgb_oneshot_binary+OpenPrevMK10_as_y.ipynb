{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aefcab43486df2128b5a3bf1d92182438885adc3"
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nfrom kaggle.competitions import twosigmanews\n\nenv = twosigmanews.make_env()\n(marketdf, newsdf) = env.get_training_data()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\n\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def prepare_data(marketdf, newsdf):\n    a bit of feature engineering\n    marketdf['time'] = marketdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    marketdf['bartrend'] = marketdf['close'] / marketdf['open']\n    marketdf['average'] = (marketdf['close'] + marketdf['open'])/2\n    marketdf['pricevolume'] = marketdf['volume'] * marketdf['close']\n    \n    newsdf['time'] = newsdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] / newsdf['sentenceCount']\n    newsdf['coverage'] = newsdf['sentimentWordCount'] / newsdf['wordCount']\n\n    # filter pre-2012 data, no particular reason\n    marketdf = marketdf.loc[marketdf['time'] > 20120000]\n    \n    # get rid of extra junk from news data\n    try:\n        droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                    'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                    'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\n        newsdf.drop(droplist, axis=1, inplace=True)\n    except:\n        print('droplist has been dropped in newsdf')\n        \n    marketdf.drop(['assetName', 'volume'], axis=1, inplace=True)\n    \n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    \n    # join news reports to market data, note many assets will have many days without news data\n    return pd.merge(marketdf, newsgp, how='left', on=['time', 'assetCode'], copy=False) #, right_on=['time', 'assetCodes'])\n\n\ndef post_scaling(df):\n    mean, std = np.mean(df), np.std(df)\n    df = (df - mean)/ (std * 2)     # original: (df - mean)/ (std * 8) \n    return np.clip(df,-1,1)\n\ndef post_scaling_nonVarScaling(df):\n    mean, std = np.mean(df), np.std(df)\n    df = (df - mean)/ (std)\n    return np.clip(df,-1,1)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89dddb79dcacb17265c18e793854d381385627df"
      },
      "cell_type": "markdown",
      "source": "# read and save all test data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5bf1e04fdc16c45eb98f0b88cd2d5d0f53fc993c"
      },
      "cell_type": "code",
      "source": "import time\nprint('=========> read all test data')\nmarket_obs_df = None\nnews_obs_df = None\npred_df = None\nevery_days = env.get_prediction_days()\nnews_droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\nload_time = 0\nfor (m_df, n_df, predictions_template_df) in every_days:\n    t_start = time.clock()\n    env.predict(predictions_template_df)\n    predictions_template_df['time'] = m_df.time.min()\n    if market_obs_df is None:\n        market_obs_df = m_df\n        news_obs_df = n_df.drop(news_droplist, axis=1)\n        pred_df = predictions_template_df\n    else:\n        market_obs_df = market_obs_df.append(m_df, ignore_index=True)\n        news_obs_df = news_obs_df.append(n_df.drop(news_droplist, axis=1), ignore_index=True)\n        pred_df = pred_df.append(predictions_template_df, ignore_index=True)\n    t_end = time.clock()\n    load_time += (t_end-t_start)\n    print('single turn time:',t_end-t_start,'total time:',load_time)\n### save test data （为了防止后面的操作给写乱了）\nprint('=========> save test data to txt file')\nfw = open('market_obs_df.txt','wb')\npickle.dump(market_obs_df, fw, -1)  \nfw.close()\nfw = open('news_obs_df.txt','wb')\npickle.dump(news_obs_df, fw, -1)  \nfw.close()\nfw = open('pred_df.txt','wb')\npickle.dump(pred_df, fw, -1)  \nfw.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "527785d214ef7b9b8910d45172e03acf1cfc9408"
      },
      "cell_type": "markdown",
      "source": "# build trainset using train data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "980a256083abfcf6d138c040a32c08e2bfdfe1ee"
      },
      "cell_type": "code",
      "source": "print('============> preparing data...')\ncdf = prepare_data(marketdf, newsdf)    \ndel marketdf, newsdf  # save the precious memory\n\nprint('============> building training set...')\ntargetcols = ['returnsOpenNextMktres10']\ntraincols = [col for col in cdf.columns if col not in ['time', 'assetCode', 'universe'] + targetcols]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "187392cfe8d8b710217dbd88e32633cba7c23eb2"
      },
      "cell_type": "code",
      "source": "### we be classifyin\n# left_threshold = -1\n# right_threshold = 1\n# ## 直接把超出left_threshold和right_threshold的数据扔掉\n# trainset_drop_index_left = cdf[cdf[targetcols[0]] < left_threshold].index\n# cdf.drop(trainset_drop_index_left, axis='index', inplace=True)\n# cdf.reset_index(drop=True)\n# trainset_drop_index_right = cdf[cdf[targetcols[0]] > right_threshold].index\n# cdf.drop(trainset_drop_index_right, axis='index', inplace=True)\n# cdf.reset_index(drop=True)\n## 采用clip方案\n# cdf[targetcols[0]].clip(left_threshold,right_threshold, inplace=True)\n\n### 作者原始的做法\ncdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)     # kaggle public kernel的数据截断方法\n\n\n### split train set and valid set\ndates = cdf['time'].unique()\ntrain = range(len(dates))[:int(0.85*len(dates))]\nval = range(len(dates))[int(0.85*len(dates)):]\n\n### train data\nXt = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[train])].values\nYt = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates[train])].values\n\n### validation data\nXv = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[val])].values\nYv = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates[val])].values\n\nprint(Xt.shape, Xv.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f701b625e2d43adc3f416615960136fc8985c271"
      },
      "cell_type": "code",
      "source": "print ('============> Training lightgbm')\n########## params for lgb (作者原始的参数)\n# params = {\"objective\" : \"binary\",\n#           \"metric\" : \"binary_logloss\",\n#           \"num_leaves\" : 125, # originally 60\n#           \"max_depth\": -1,\n#           \"learning_rate\" : 0.0005,   # originally .01\n#           \"bagging_fraction\" : 0.9,  # subsample\n#           \"feature_fraction\" : 0.9,  # colsample_bytree\n#           \"bagging_freq\" : 5,        # subsample_freq\n#           \"bagging_seed\" : 2018,\n#           \"verbosity\" : -1 }\n\n########## params for lgb \nparams = {\"objective\" : \"binary\",\n          \"metric\" : \"binary_logloss\",\n        #   \"lambda_l1\": 0.01,\n          \"num_leaves\" : 255, # originally 60\n          \"max_depth\": -1,\n          \"learning_rate\" : 0.0005,   # originally .01\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 2018,\n          \"verbosity\" : -1 }\n\n\n## We can introduce other boosting algos, default is traditional gradient boosting decision tree\n## Other options include random forest (rf), dropouts meet multiple additive regression trees (dart), \n## or gradient-based on one-side sampling \n\nlgtrain, lgval = lgb.Dataset(Xt, Yt[:,0]), lgb.Dataset(Xv, Yv[:,0])\nlgbmodel = lgb.train(params, lgtrain, 2600, valid_sets=[lgtrain, lgval], early_stopping_rounds=300, verbose_eval=200)\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7da5233d6a481a8f38d72a51ce8aeba8e47174c4"
      },
      "cell_type": "markdown",
      "source": "# retrain model using testset data"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "35c88e5ae45b7ec2e983875790725faca3ed061c"
      },
      "cell_type": "code",
      "source": "def prepare_data_from_testset(marketdf, newsdf):\n    # a bit of feature engineering\n    marketdf['time'] = marketdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    marketdf['bartrend'] = marketdf['close'] / marketdf['open']\n    marketdf['average'] = (marketdf['close'] + marketdf['open'])/2\n    marketdf['pricevolume'] = marketdf['volume'] * marketdf['close']\n    \n    # test集合上的returnsOpenNextMktres10没有用，直接扔掉\n    marketdf.drop(['returnsOpenNextMktres10'],axis=1)\n    marketdf['new_returnsOpenNextMktres10'] = marketdf.groupby(['assetCode'])['returnsOpenPrevMktres10'].shift(-11).fillna(0)\n    \n    newsdf['time'] = newsdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] / newsdf['sentenceCount']\n    newsdf['coverage'] = newsdf['sentimentWordCount'] / newsdf['wordCount']\n\n    # filter pre-2012 data, no particular reason\n    marketdf = marketdf.loc[marketdf['time'] > 20120000]\n    # get rid of extra junk from news data\n    try:\n        droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                    'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                    'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\n        newsdf.drop(droplist, axis=1, inplace=True)\n    except:\n        print('droplist has been dropped in newsdf')\n        \n    marketdf.drop(['assetName', 'volume'], axis=1, inplace=True)\n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    # join news reports to market data, note many assets will have many days without news data\n    return pd.merge(marketdf, newsgp, how='left', on=['time', 'assetCode'], copy=False) #, right_on=['time', 'assetCodes'])\n\n\n\nprint(\"============> training model with test_set data...\")\ntry:\n    del cdf\nexcept:\n    print('rerun! cdf has been deleted')\ncdf = prepare_data_from_testset(market_obs_df, news_obs_df)\n\ntry:\n    del market_obs_df, news_obs_df, pred_df\nexcept:\n    print('rerun! market_obs_df, news_obs_df, pred_df have been deleted')\n\ntry:\n    del lgtrain, lgval\nexcept:\n    print('rerun! lgtrain, lgval have been deleted')    \n\nprint(\"---------------------------> building train data from test set...\")\ntargetcols = ['new_returnsOpenNextMktres10'] # 将这一列直接作为y\ntraincols = [col for col in cdf.columns if col not in ['time', 'assetCode', 'universe'] + targetcols]\n### 作者原始的做法\ncdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)     # kaggle public kernel的数据截断方法\n### split train set and valid set\ndates = cdf['time'].unique()\ntrain = range(len(dates))[:int(0.90*len(dates))]\nval = range(len(dates))[int(0.90*len(dates)):]\n### train data\nXt = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[train])].values\nYt = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates[train])].values\n### validation data\nXv = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates[val])].values\nYv = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates[val])].values\n\nprint(\"---------------------------> train set built from test set:\", Xt.shape, Xv.shape)\nprint(\"---------------------------> train model from new train-set:\", Xt.shape, Xv.shape)\nlgtrain, lgval = lgb.Dataset(Xt, Yt[:,0]), lgb.Dataset(Xv, Yv[:,0])\nlgbmodel = lgb.train(params, lgtrain, 2000, valid_sets=[lgtrain, lgval], early_stopping_rounds=300, verbose_eval=200)\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "089685c7e7fa2c8a03ef0b6913d63fd6539f2894"
      },
      "cell_type": "markdown",
      "source": "# begin prediction"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06925429a914847a71676e9d9a577d3c93882646"
      },
      "cell_type": "code",
      "source": "######## 新的prediction code，由于env.get_prediction_days()只能调用一次，因此改成前面全读进来，在此对数据进行使用\nprint(\"============> generating predictions...\")\ntry:\n    del cdf\nexcept:\n    print('rerun! cdf has been deleted')\n\nimport gc\ndel lgtrain, lgval\ngc.collect()\nprint('read test data from txt file...')\nfr = open('market_obs_df.txt','rb')  \nmarket_obs_df = pickle.load(fr)  \nfr.close()\nfr = open('news_obs_df.txt','rb')  \nnews_obs_df = pickle.load(fr)  \nfr.close()\nfr = open('pred_df.txt','rb')  \npred_df = pickle.load(fr)  \nfr.close()\n\n###### 开始进行预测\ncdf = prepare_data(market_obs_df, news_obs_df)\ndel market_obs_df, news_obs_df\nXp = cdf[traincols].fillna(0).values\npreds = lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration)*2 - 1\npredsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})\npredtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\npredtemplatedf.head(n=5)\npredtemplatedf[['time', 'assetCode', 'confidenceValue']].to_csv('self_buld_submission.csv', index=False, float_format='%.8f')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08640fd1c2bf4a4ddd7ca1ae5202b5413d5eaeb5"
      },
      "cell_type": "code",
      "source": "\n# ########## 原来的prediction code，按照天来预测，一天天的预测\n# print(\"============> generating predictions...\")\n# try:\n#     del cdf\n# except:\n#     print('cdf has been deleted')\n\n# preddays = env.get_prediction_days()\n# th_ = 0\n# for marketdf, newsdf, predtemplatedf in preddays:\n#     # print('------- |',th_,'day')\n#     cdf = prepare_data(marketdf, newsdf)\n#     Xp = cdf[traincols].fillna(0).values\n#     preds = lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration)*2 - 1\n#     # predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})       # 原始程序中，对preds进行了中心化\n#     predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})\n#     predtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\n#     env.predict(predtemplatedf)\n#     th_ += 1 \n\n# ########## \n# print(\"============> writing predictions...\")\n# env.write_submission_file()\n# print(\"============> all finished ...\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c64f279538cf3dcf68e2776562060835917f58b"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e256a3589430cf5e3af9a46dd072eb4067fe15d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}