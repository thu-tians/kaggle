{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nfrom kaggle.competitions import twosigmanews\n\nenv = twosigmanews.make_env()\n(marketdf, newsdf) = env.get_training_data()\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\n\ndef prepare_data(marketdf, newsdf):\n    # a bit of feature engineering\n    marketdf['time'] = marketdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    marketdf['bartrend'] = marketdf['close'] / marketdf['open']\n    marketdf['average'] = (marketdf['close'] + marketdf['open'])/2\n    marketdf['pricevolume'] = marketdf['volume'] * marketdf['close']\n    \n    newsdf['time'] = newsdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] / newsdf['sentenceCount']\n    newsdf['coverage'] = newsdf['sentimentWordCount'] / newsdf['wordCount']\n\n    # filter pre-2012 data, no particular reason\n    marketdf = marketdf.loc[marketdf['time'] > 20120000]\n    \n    # get rid of extra junk from news data\n    droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\n    newsdf.drop(droplist, axis=1, inplace=True)\n    marketdf.drop(['assetName', 'volume'], axis=1, inplace=True)\n    \n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    \n    # join news reports to market data, note many assets will have many days without news data\n    return pd.merge(marketdf, newsgp, how='left', on=['time', 'assetCode'], copy=False) #, right_on=['time', 'assetCodes'])\n\n\ndef load_multi_model():\n    model_list = []\n    for i in range(n_splits):\n        model_list.append(lgb.Booster(model_file='model' + str(i)))\n    return model_list\n\n\ndef post_scaling(df):\n    mean, std = np.mean(df), np.std(df)\n    df = (df - mean)/ (std * 8)\n    return np.clip(df,-1,1)\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "####################### params for lgb #######################\nn_splits = 5\nseed = 2018\n\n### (binary version, 只能处理0、1的分类)\n# params = {\"objective\" : \"binary\",\n#           \"metric\" : \"binary_logloss\",\n#           \"num_leaves\" : 125, # originally 60\n#           \"max_depth\": -1,\n#           \"learning_rate\" : 0.0005,   # originally .01\n#           \"bagging_fraction\" : 0.9,  # subsample\n#           \"feature_fraction\" : 0.9,  # colsample_bytree\n#           \"bagging_freq\" : 5,        # subsample_freq\n#           \"bagging_seed\" : 2018,\n#           \"verbosity\" : -1 }\n\n#### (rmse method)\nparams = {\"objective\" : \"regression\",\n            \"metric\" : \"rmse\",\n            \"lambda_l2\": 0,\n            \"num_leaves\" : 256, # originally 60\n            \"max_depth\": -1,\n            \"learning_rate\" : 0.005,   # originally .01\n            \"bagging_fraction\" : 0.9,  # subsample\n            \"feature_fraction\" : 0.9,  # colsample_bytree\n            \"bagging_freq\" : 5,        # subsample_freq\n            \"bagging_seed\" : 2018,\n            \"verbosity\" : -1 }\nprint('============> have read params')\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9953d8c218b14a227e6dd8f18701a60c56c49e05"
      },
      "cell_type": "code",
      "source": "print('============> preparing data...')\ncdf = prepare_data(marketdf, newsdf)    \ndel marketdf, newsdf  # save the precious memory",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e259df301827095c9e4d5203d79fba9c4ea5e0af"
      },
      "cell_type": "code",
      "source": "print('============> building training set...')\ntargetcols = ['returnsOpenNextMktres10']\ntraincols = [col for col in cdf.columns if col not in ['time', 'assetCode', 'universe'] + targetcols]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9d426a3cd65b6a0b78cb77c23ae15e7b731d1792"
      },
      "cell_type": "code",
      "source": "### we be classifyin\n# cdf[targetcols[0]] = (cdf[targetcols[0]] > 0).astype(int)     # kaggle public kernel的数据截断方法\noriginal_cdf_length = len(cdf.index)\nleft_threshold = -0.4\nright_threshold = 0.4\ntrainset_drop_index_left = cdf[cdf[targetcols[0]] < left_threshold].index\ncdf.drop(trainset_drop_index_left, axis='index', inplace=True)\ncdf.reset_index(drop=True)\ntrainset_drop_index_right = cdf[cdf[targetcols[0]] > right_threshold].index\ncdf.drop(trainset_drop_index_right, axis='index', inplace=True)\ncdf.reset_index(drop=True)\nprint('------------ nows cdf/original cdf length =',len(cdf.index)/original_cdf_length)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e54796a12df9ec824aad06ddf1c58a4ed2eda9d5"
      },
      "cell_type": "code",
      "source": "#### 根据unique day来划分train-test集合\n# dates_unique = cdf['time'].unique()\n# train = range(len(dates_unique))[:int(0.85*len(dates_unique))]\n# val = range(len(dates_unique))[int(0.85*len(dates_unique)):]\n#### train data （by unique day)\n# Xt = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates_unique[train])].values\n# Yt = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates_unique[train])].values\n#### validation data  （by unique day)\n# Xv = cdf[traincols].fillna(0).loc[cdf['time'].isin(dates_unique[val])].values\n# Yv = cdf[targetcols].fillna(0).loc[cdf['time'].isin(dates_unique[val])].values\n# print(Xt.shape, Xv.shape)\n\n### split train set and valid set\nX = cdf[traincols].fillna(0).copy()\ny = cdf[targetcols].fillna(0).copy()\ndel cdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "42b27e51cdce744cb3e084d122ac03b98c9143c1"
      },
      "cell_type": "code",
      "source": "kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\nindex = 0\nfor train_index, test_index in kf.split(X):\n    print('---------------------- now fold =', index, 'in', n_splits, '----------------------')\n    X_train, X_valid, y_train, y_valid = X.iloc[train_index], X.iloc[test_index], y.iloc[train_index], y.iloc[test_index]\n    print ('============> Training lightgbm')\n    # We can introduce other boosting algos, default is traditional gradient boosting decision tree\n    # Other options include random forest (rf), dropouts meet multiple additive regression trees (dart), \n    # or gradient-based on one-side sampling \n    lgtrain, lgval = lgb.Dataset(X_train, label=y_train), lgb.Dataset(X_valid, label=y_valid)\n    lgbmodel = lgb.train(params, lgtrain, 1500, valid_sets=[lgval], early_stopping_rounds=300, verbose_eval=200)\n    lgbmodel.save_model('model' + str(index))\n    print('============> saved ' + str(index))\n    index += 1\n\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e13fcfd2475a1c6ceee9bd9a89291b68cdccaf89"
      },
      "cell_type": "code",
      "source": "print(\"============> generating predictions...\")\nmodel_list = load_multi_model()\npreddays = env.get_prediction_days()\nfor marketdf, newsdf, predtemplatedf in preddays:\n    cdf = prepare_data(marketdf, newsdf)\n    Xp = cdf[traincols].fillna(0).values\n    # preds = lgbmodel.predict(Xp, num_iteration=lgbmodel.best_iteration) * 2 - 1   # 原始的预测程序，对应的是0-1分类的情况\n\n    for i in range(len(model_list)):\n        if i == 0:\n            preds = model_list[i].predict(Xp, num_iteration=model_list[i].best_iteration)\n        else:\n            preds = preds + model_list[i].predict(Xp, num_iteration=model_list[i].best_iteration)\n\n    preds = preds/n_splits\n    predsdf = pd.DataFrame({'ast':cdf['assetCode'],'conf':post_scaling(preds)})\n    predtemplatedf['confidenceValue'][predtemplatedf['assetCode'].isin(predsdf.ast)] = predsdf['conf'].values\n    env.predict(predtemplatedf)\n\n\n\n# ouput final file    \nenv.write_submission_file()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "772733e5ef85fa6b4ecd10a50d0e52edf7c963ab"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}